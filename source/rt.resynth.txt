
/*

Definitions:
-----------

- vocoder:
apply the amplitudes of frequency bands of the voice to modulate another sound, using the same bands and volumes.
To do this I would need:

Vocoder<Osc> =
(Voice (read sample from a queue at each step) -> Bandfilter(s) -> Envelope follower(s))
Osc -> Bandfilter(s) -> Modulate by voice envelopes -> Sum

for example:
Vocoder<MultiEnveloped<SineOsc>>
*/

/* Backlog:

 - spread frequencies in stereo field (low = left, high = right)
 . on note change, the spread should change, at the same rate than frequency, so the spread is a function of oscillator's angleIncrements
 but volume of channel can only change every 16 samples! so we need to
 make it possible for an audioelement to work with 2 channels:
 StereoPanner<Osc>
 
 FinalAudioElement has a AEBuffer
 FinalAudioElement could deduce "int nAudioOut" from the Algo
 and do AEBuffer<int nAudioOut>

 Note that the notion of channel seems redundant for our use case.
 The notion of channel is usefull:
 - to have buffering
 . but we have buffering also in FinalAudioElement so it is not useful
 - when there are _overlapping_ sound requests with crossfades of same length, and no envelope
 . but on audioelement we have envelopes so there is no need for it
 - to apply a volume
 . but on audioelement we have volumes too so there is no need for it
 - to easily shutdown audio with a nice fade out
 . It is overkill to use channel "just for that", we could do the same thing at a higher level
 
 So instead of looking in the channel for frames, we can look directly in the currently active audioelement(s).
 
 This will also simplify the implementation, because the notion of channel was hard to grasp.

 We could even think of a mode where the audioelement directly add its data to the output signal, to skip its own buffers
 
 To maintain compatibility with requests, we can wrap a channel in an audioelement (we will need that until all requests are converted to audioElements)

- in a unit test, verify the effect of signal phase on peak magnitude (I expect that this has an influence on lower frequencies)

 ----------------
 Creative effects
 ----------------
 
 - adjust stereo spread in real time.
 Note we could have in StereoPanned = setup to change abruptly the pan, move_to to change the pan with an interpolation

 - via volume tracking only (not envelope), make the volume decrease slower than in reality (notion of meta envelope)
    
 - use many harmonizers, pre / post autotune (use midi input to define where the harmonizer is?)
 
 - autotune global mode to say how long it takes to go to the autotuned frequency, i.e
 if 0 : on note one, autotune
 if > 0: on note on, use the original frequency, then if the pitch is detected again the next time, autotune it and issue a note change (do not take into account max pitch tracking distance)

 - only active for "autotune chord",
 when "harmonics" is active:
 . sliders "count sub harmonics" / "count harmonics"
 . checkbox to make harmonics match an integer midi pitch
 when "octave replication" is active:
 . sliders "count sub octaves" / "count upper octaves"
 
 - record some presets
 . save as json
 . hints to make make_presets:
 .. max_pitch = 100 makes speec more intelligible, however we need max_pitch = 150 for robotic effects in the high end.
 .. an interesting "frequency randomization" effect happens with
 ... low frequency voice
 ... small analysis window size (so that the frequency is not well found, hence the randomization effect)
 ... "major chord + subharmonics" autotune (so that the frequencies are random but still "musically pleasing")

 - an interesting robotic effect occurs with feedback and varying autotune interval size :
 the fact that the interval size changes breaks the feedback loop so the variation should be fast enough so that the sound will not saturate.
 Hence we need to automate, using oscillators and ranges, meta parameter variation.

 - pitch shifter
 . use double fft to detect main pitch, shift pitch according to midi played notes
 (if many notes are played a duplication ocurs)

 - implement a "slow down" :
 use 2 windows, drop 2 windows, etc...
 advance at a 4th of the speed, instead of at the normal speed
 
 - make an arpegiating effect where current frequencies are played individually in
 sequence from bottom to top.
 varying parameters :
 . arpegiating speed
 . arpegiating overlap
 . gap
 . sustain
 . one-way / 2-ways / 2-ways-bounds-exclusive
 ---
 ...---
 ......---
 or
 ---
 .---
 ..---
 or
 ---
 ......---
 ............---
 or
 --- --- ---  (gap = 1, one-way)
 .-- .-- .--
 ..- ..- ..-
 or
 --- ..- ---  (gap = 1, 2-ways)
 .-- .-- .--
 ..- --- ..-
 or
 ---..----   (gap = 0, 2-ways)
 .--.--.--
 ..----..-
 or
 --..--.     (gap = 0, 2-ways-bounds-exclusive)
 .-.-.-.
 ..--..-
 or
 --.--.--.  (gap = 1, sustain=2, one-way-infinite)
 .--.--.--
 -.--.--.-

 We can detect which notes begin together, to have the notion of chord, or simply consider that all playing notes are part of the same chord
 
 - control AHDSR parameters in realtime

 - an interesting effect occurs when reinjecting a delayed version of the output in the input,
 it was discovered by using the "external" feedback from speakers to mic, and by delaying the input signal by one second.
 to implement "internal" feedback, we need additional parameters:
 feedback volume:
 feedback delay:
 and a queue where the producer is the audio output thread, the consumer is the analysis thread.
 The analysis thread reads from both the input and the output queue.
 If the latencies for in and out are different, the sizes of the queues should be augmented by the number of samples corresponding to the max latency

 - customize resynthesys:
 Add a curve 0..1 -> 0..1 to change (compress / expand) volumes
 Add a filter notion : "volume(freq)" to allow to low-pass or high-pass for example

 --
 UI
 --
 
 - use log scales for ui params
 analysis period,
 analysis window size,
 min volume,
 pitch interval,
 pitch tracking
 
 - Display in UI :
 FFT size in frames,
 window size in frames,
 stride in frames
 
 ------
 Others
 ------
 
 - perf : sort/unique the vector used in find_closest_pitch, use binary search.

 - Find a way to that verify notechanges, noteon, noteoff take midi time into account
 (output buffers have a limited size so we can't easily test by using long output buffers)
 
 - Improve memory locality of audio data by using
 . a move-only wrapper around a std::vector for audio queues
  (check the stream latency and use that value to compute the size of vectors)
 . a wrapper around the queue to access elements one sample at a time
 
 - (micro optimization) 'samples' has size = "window size"
 this choice was made to improve memory locality but
 when stride is very small, copying the overlap after each processing
 can become significant : for a stride = 1, for each audio frame, we copy "window size" audio frames!
 if 'samples' had twice its size, we would copy, on average, a single frame per audio frame which is much more reasonable.

 --------------
 Peak detection
 --------------
 
 - Improve accuracy of low frequency detection, using small and large ffts:
 small ffts will be used for high frequencies (good temporal accuracy)
 and large ffts will be used for low frequencies (poor temporal accuracy)
 
 - there is a method where using the derivative of the audio signal we get a more precise fourier transform :
 https://hal.archives-ouvertes.fr/hal-00308047/document
 However I'm not sure it will help, in the paper they don't seem to know about quadratic interpolation for peak finding?
 
 - when detecting peaks, to have better precision on close peaks, we could iteratively remove the highest peak contribution from the spectrum.
 But it's not an easy task : when 2 peaks are close, they interfere with each other so ideally we should iterativelly optimize the pair of peaks.
 cf. TEST(ParabollaGaussian, test)
 
 Then we can use these parameters:
 
 static PitchReductionMethod constexpr pitch_method =
 PitchReductionMethod::PonderateByVolume;
 static VolumeReductionMethod constexpr volume_method =
 VolumeReductionMethod::SumVolumes;
 
 - write unit tests for frequency detection:
 . generate a signal with known frequencies / amplitudes.
 . for every type of window, detect peaks
 

 */



/*
 -----------------
 Development Notes
 -----------------

 - We are using portaudio HEAD that fixes a bug when the same device is used as input and output
 (the bug was that buffer sizes had to be identical)
 
 */



/*
 --------------------------------------
 About jitter (and how we cope with it)
 --------------------------------------
 
 Here is what is likely to happen with threads jitter, using an example.
 
 To simplify, we will assume that each audio input callback call handles 'widow_center_stride' input samples,
 and that the input signal contains 4 events (0, 1, 2, 3) which are exactly 'window_center_stride' apart, like so:
 
 input signal: 0--1--2--3--
 
 We have 3 threads running: the audio input thread, the processing thread and the audio output thread.
 The audio input thread writes in a queue read by the process thread, and
 the process thread writes in a queue read by the audio output thread.
 
 In an ideal world, all threads run at the same regular intervals, with no overlap:
 
 audio in thread :    0--      1--      2--      3--       // we represent both the input signal and the time it takes for the input thread to run
 process thread  :       ---      ---      ---      ---    // we represent the time it takes for the thread to run
 audio out thread:          0--      1--      2--      3-- // we represent both the output signal and the time it takes for the output thread to run
 
 -> output signal : 0--1--2--3--
 
 In this ideal case, the events maintain their timings. But let's see how we can diverge from the ideal case:
 
 A. We can imagine that the 'process' thread has jitter:
 
 audio in thread :    0--      1--      2--      3--
 process thread  :       ---                ---     ---
 audio out thread:          0--      ---      1-2      3--
 
 -> output signal : 0-----1-23--
 
 B. We can imagine that the 'process' thread runs less often that the audio threads:
 
 audio in thread :    0--      1--      2--      3--
 process thread  :       ---               ---
 audio out thread:          0--      ---      12-      3--
 
 -> output signal : 0-----12-3--
 
 C. we can imagine that the audio in thread has jitter:
 
 audio in thread :    0--      1--            2--3--
 process thread  :       ---      ---      ---      ---
 audio out thread:          0--      1--      ---      23-
 
 -> output signal : 0--1-----23-
 
 D. we can imagine that the audio out thread has jitter:
 
 audio in thread :    0--      1--      2--      3--
 process thread  :       ---      ---      ---      ---
 audio out thread:          0--      1--            2-3---
 
 -> output signal : 0--1--2-3---
 
 In the real world, it is likely that A., B. C. and D. happen simulataneously.
 
 To cope with this, we use midi timestamps to accurately
 trigger the relevant actions at the right time in the audio out thread.
 */

