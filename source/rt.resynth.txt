
/*

- use a feedback loop : output -> analysis
implemented as a generic post processing step "write to queue""

*/

/* Backlog:

- in a unit test, verify the effect of signal phase on peak magnitude (I expect that this has an influence on lower frequencies)

------------
Oscillators:
------------

- bandlimit oscillators to avoid aliasing:
https://courses.cs.washington.edu/courses/cse490s/11au/Readings/Digital_Sound_Generation_1.pdf
https://ccrma.stanford.edu/~juhan/vas.html
https://www.music.mcgill.ca/~gary/307/week5/bandlimited.html
https://ccrma.stanford.edu/~stilti/papers/blit.pdf

For square, triangle, saw : simply use additive synthesis

- view waveform for carrier, modulator, analysis, sum
sync on ascending zero crossing

- add harmonics (for each waveform)

- it would be nice to have some detune to have more phasing of oscillators

--------
Vocoder:
-------

- Change interactively:
. the window shape for modulator
. the method to compute the bands (max or sum)

- Do we need some low pass filtering on the band coefficients?

- multilevel fft to have fast reactions for high frequencies, and better slower detetction of low frequencies

- https://www.soundonsound.com/techniques/introduction-esps-vocoders
in one band, use white noise to make speech more intelligible

 ----------------
 Creative effects
 ----------------
 
  - record some presets
 . save as json
 . save last state every second
 . hints to make make_presets:
 .. max_pitch = 100 makes speec more intelligible, however we need max_pitch = 150 for robotic effects in the high end.
 .. an interesting "frequency randomization" effect happens with
 ... low frequency voice
 ... small analysis window size (so that the frequency is not well found, hence the randomization effect)
 ... "major chord + subharmonics" autotune (so that the frequencies are random but still "musically pleasing")

 - an interesting robotic effect occurs with feedback and varying autotune interval size :
 the fact that the interval size changes breaks the feedback loop so the variation should be fast enough so that the sound will not saturate.
 Hence we need to automate, using oscillators and ranges, meta parameter variation.

 - an interesting effect occurs when reinjecting a delayed version of the output in the input,
 it was discovered by using the "external" feedback from speakers to mic, and by delaying the input signal by one second.
 to implement "internal" feedback, we need additional parameters:
 feedback volume:
 feedback delay:
 and a queue where the producer is the audio output thread, the consumer is the analysis thread.
 The analysis thread reads from both the input and the output queue.
 If the latencies for in and out are different, the sizes of the queues should be augmented by the number of samples corresponding to the max latency

 - randomly change stereo spread of currently played notes at the beginning of every analysis frame (randomize whether the pan changes or not, and by how much).
 . in StereoPanned, setup changes abruptly the pan, we could have a "move_to" method to change the pan with an interpolation (we need to maintain equal power during the interpolation)

 - via volume tracking only (not envelope), make the volume decrease slower than in reality (notion of meta envelope)
    
 - use many harmonizers, pre / post autotune (use midi input to define where the harmonizer is?)
 
 - autotune global mode to say how long it takes to go to the autotuned frequency, i.e
 if 0 : on note on, autotune
 if > 0:
 .on note on, use the original frequency, then if the pitch is detected again the next time, autotune it and issue a note change (do not take into account max pitch tracking distance)
 .or issue both a noteon and a notechange with a delay in the midi timestamp (we know what is the future stride so it is easy to make this delay smaller)

 - only active for "autotune chord",
 when "harmonics" is active:
 . sliders "count sub harmonics" / "count harmonics"
 . checkbox to make harmonics match an integer midi pitch
 when "octave replication" is active:
 . sliders "count sub octaves" / "count upper octaves"
 
 - pitch shifter
 . use double fft to detect main pitch, shift pitch according to midi played notes
 (if many notes are played a duplication ocurs)

 - implement a "slow down" :
 use 2 windows, drop 2 windows, etc...
 advance at a 4th of the speed, instead of at the normal speed
 
 - make an arpegiating effect where current frequencies are played individually in
 sequence from bottom to top.
 varying parameters :
 . arpegiating speed
 . arpegiating overlap
 . gap
 . sustain
 . one-way / 2-ways / 2-ways-bounds-exclusive
 ---
 ...---
 ......---
 or
 ---
 .---
 ..---
 or
 ---
 ......---
 ............---
 or
 --- --- ---  (gap = 1, one-way)
 .-- .-- .--
 ..- ..- ..-
 or
 --- ..- ---  (gap = 1, 2-ways)
 .-- .-- .--
 ..- --- ..-
 or
 ---..----   (gap = 0, 2-ways)
 .--.--.--
 ..----..-
 or
 --..--.     (gap = 0, 2-ways-bounds-exclusive)
 .-.-.-.
 ..--..-
 or
 --.--.--.  (gap = 1, sustain=2, one-way-infinite)
 .--.--.--
 -.--.--.-

 We can detect which notes begin together, to have the notion of chord, or simply consider that all playing notes are part of the same chord
 
 - control AHDSR parameters in realtime

 - customize resynthesys:
 Add a curve 0..1 -> 0..1 to change (compress / expand) volumes
 Add a filter notion : "volume(freq)" to allow to low-pass or high-pass for example

 --
 UI
 --
 
 - use log scales for ui params
 analysis period,
 analysis window size,
 min volume,
 pitch interval,
 pitch tracking
 
 ------
 Others
 ------

 - Improve memory locality of audio data by using
 . a move-only wrapper around a std::vector for audio queues
  (check the stream latency and use that value to compute the size of vectors)
 . a wrapper around the queue to access elements one sample at a time
 
 - (micro optimization) 'samples' has size = "window size"
 this choice was made to improve memory locality but
 when stride is very small, copying the overlap after each processing
 can become significant : for a stride = 1, for each audio frame, we copy "window size" audio frames!
 if 'samples' had twice its size, we would copy, on average, a single frame per audio frame which is much more reasonable.

 - Find a way to that verify notechanges, noteon, noteoff take midi time into account
 (output buffers have a limited size so we can't easily test by using long output buffers)
 
 --------------
 Peak detection
 --------------
 
 - Improve accuracy of low frequency detection, using small and large ffts:
 small ffts will be used for high frequencies (good temporal accuracy)
 and large ffts will be used for low frequencies (poor temporal accuracy)
 
 - there is a method where using the derivative of the audio signal we get a more precise fourier transform :
 https://hal.archives-ouvertes.fr/hal-00308047/document
 However I'm not sure it will help, in the paper they don't seem to know about quadratic interpolation for peak finding?
 
 - when detecting peaks, to have better precision on close peaks, we could iteratively remove the highest peak contribution from the spectrum.
 But it's not an easy task : when 2 peaks are close, they interfere with each other so ideally we should iterativelly optimize the pair of peaks.
 cf. TEST(ParabollaGaussian, test)
 
 Then we can use these parameters:
 
 static PitchReductionMethod constexpr pitch_method =
 PitchReductionMethod::PonderateByVolume;
 static VolumeReductionMethod constexpr volume_method =
 VolumeReductionMethod::SumVolumes;
 
 - write unit tests for frequency detection:
 . generate a signal with known frequencies / amplitudes.
 . for every type of window, detect peaks
 

 */



/*
 -----------------
 Development Notes
 -----------------

 - We are using portaudio HEAD that fixes a bug when the same device is used as input and output
 (the bug was that buffer sizes had to be identical)
 
 */



/*
 --------------------------------------
 About jitter (and how we cope with it)
 --------------------------------------
 
 Here is what is likely to happen with threads jitter, using an example.
 
 To simplify, we will assume that each audio input callback call handles 'widow_center_stride' input samples,
 and that the input signal contains 4 events (0, 1, 2, 3) which are exactly 'window_center_stride' apart, like so:
 
 input signal: 0--1--2--3--
 
 We have 3 threads running: the audio input thread, the processing thread and the audio output thread.
 The audio input thread writes in a queue read by the process thread, and
 the process thread writes in a queue read by the audio output thread.
 
 In an ideal world, all threads run at the same regular intervals, with no overlap:
 
 audio in thread :    0--      1--      2--      3--       // we represent both the input signal and the time it takes for the input thread to run
 process thread  :       ---      ---      ---      ---    // we represent the time it takes for the thread to run
 audio out thread:          0--      1--      2--      3-- // we represent both the output signal and the time it takes for the output thread to run
 
 -> output signal : 0--1--2--3--
 
 In this ideal case, the events maintain their timings. But let's see how we can diverge from the ideal case:
 
 A. We can imagine that the 'process' thread has jitter:
 
 audio in thread :    0--      1--      2--      3--
 process thread  :       ---                ---     ---
 audio out thread:          0--      ---      1-2      3--
 
 -> output signal : 0-----1-23--
 
 B. We can imagine that the 'process' thread runs less often that the audio threads:
 
 audio in thread :    0--      1--      2--      3--
 process thread  :       ---               ---
 audio out thread:          0--      ---      12-      3--
 
 -> output signal : 0-----12-3--
 
 C. we can imagine that the audio in thread has jitter:
 
 audio in thread :    0--      1--            2--3--
 process thread  :       ---      ---      ---      ---
 audio out thread:          0--      1--      ---      23-
 
 -> output signal : 0--1-----23-
 
 D. we can imagine that the audio out thread has jitter:
 
 audio in thread :    0--      1--      2--      3--
 process thread  :       ---      ---      ---      ---
 audio out thread:          0--      1--            2-3---
 
 -> output signal : 0--1--2-3---
 
 In the real world, it is likely that A., B. C. and D. happen simulataneously.
 
 To cope with this, we use midi timestamps to accurately
 trigger the relevant actions at the right time in the audio out thread.
 */

